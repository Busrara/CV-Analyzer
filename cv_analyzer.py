# -*- coding: utf-8 -*-
"""Welcome To Colab

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/notebooks/intro.ipynb
"""

import os
import re
import nltk
import pandas as pd
import PyPDF2
import spacy
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from collections import Counter
import matplotlib.pyplot as plt
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

# Download necessary NLTK data
nltk.download('punkt')
nltk.download('stopwords')

# Load English language model for NLP
try:
    nlp = spacy.load("en_core_web_sm")
except:
    print("Downloading spaCy English model...")
    os.system("python -m spacy download en_core_web_sm")
    nlp = spacy.load("en_core_web_sm")

class CVAnalyzer:
    def __init__(self):
        self.stop_words = set(stopwords.words('english'))
        # Common skill-related terms to look for
        self.skill_indicators = [
            "experience", "skill", "knowledge", "proficient", "familiar",
            "years", "expertise", "background", "capability", "competent",
            "trained", "qualified", "certified", "degree", "developed",
            "built", "created", "managed", "led", "designed", "implemented"
        ]

    def read_pdf(self, pdf_path):
        """Extract text from a PDF file"""
        pdf_text = ""
        try:
            with open(pdf_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                for page_num in range(len(pdf_reader.pages)):
                    pdf_text += pdf_reader.pages[page_num].extract_text()
        except Exception as e:
            print(f"Error reading PDF: {str(e)}")
            return ""
        return pdf_text

    def read_file(self, file_path):
        """Read text from PDF or TXT file"""
        if file_path.endswith('.pdf'):
            return self.read_pdf(file_path)
        elif file_path.endswith('.txt'):
            try:
                with open(file_path, 'r', encoding='utf-8') as file:
                    return file.read()
            except UnicodeDecodeError:
                # Try with different encoding if utf-8 fails
                with open(file_path, 'r', encoding='latin-1') as file:
                    return file.read()
        else:
            raise ValueError("Unsupported file format. Please use PDF or TXT files.")

    def extract_keywords(self, input_text):
        """Extract important keywords from text using NLP"""
        # Ensure input_text is a string to avoid errors
        if not isinstance(input_text, str):
            input_text = str(input_text)

        if not input_text.strip():
            return []

        # Process with spaCy
        doc = nlp(input_text.lower())
        keywords = []

        # Extract nouns, proper nouns, and adjectives that might represent skills or requirements
        for token in doc:
            if (token.pos_ in ["NOUN", "PROPN", "ADJ"] and
                token.text not in self.stop_words and
                len(token.text) > 2):
                keywords.append(token.text)

        # Extract named entities (especially for technologies, organizations, etc.)
        for ent in doc.ents:
            if ent.label_ in ["ORG", "PRODUCT", "LANGUAGE"]:
                keywords.append(ent.text.lower())

        # Look for skill indicators
        for chunk in doc.noun_chunks:
            for skill in self.skill_indicators:
                if skill in chunk.text.lower():
                    cleaned_chunk = ' '.join([token.text for token in chunk if token.text not in self.stop_words])
                    if cleaned_chunk:
                        keywords.append(cleaned_chunk)

        # Extract common programming languages, tools, and technologies
        tech_patterns = [
            r'\b(?:python|java|javascript|js|c\+\+|c#|ruby|php|swift|kotlin|go|rust|typescript|html|css|sql)\b',
            r'\b(?:react|angular|vue|node\.js|django|flask|spring|express|tensorflow|pytorch|scikit-learn)\b',
            r'\b(?:aws|azure|gcp|docker|kubernetes|jenkins|git|jira|agile|scrum|kanban)\b',
            r'\b(?:machine learning|artificial intelligence|data science|big data|cloud computing|devops)\b'
        ]

        for pattern in tech_patterns:
            matches = re.findall(pattern, input_text.lower())
            keywords.extend(matches)

        return list(set(keywords))

    def calculate_similarity(self, cv_text, job_desc):
        """Calculate similarity between CV and job description"""
        # Use CountVectorizer to convert texts to numerical vectors
        vectorizer = CountVectorizer().fit_transform([cv_text.lower(), job_desc.lower()])
        vectors = vectorizer.toarray()

        # Calculate cosine similarity
        similarity = cosine_similarity(vectors[0].reshape(1, -1), vectors[1].reshape(1, -1))[0][0]
        return similarity * 100  # Convert to percentage

    def identify_missing_keywords(self, cv_keywords, job_keywords):
        """Identify keywords present in job description but missing in CV"""
        missing_keywords = [kw for kw in job_keywords if kw not in cv_keywords]
        return missing_keywords

    def generate_report(self, cv_path, job_description):
        """Generate comprehensive analysis report"""
        # Read CV
        cv_text = self.read_file(cv_path)

        if not cv_text.strip():
            raise ValueError("Could not read CV file or CV file is empty")

        # Extract keywords
        cv_keywords = self.extract_keywords(cv_text)
        job_keywords = self.extract_keywords(job_description)

        # Calculate similarity
        match_percentage = self.calculate_similarity(cv_text, job_description)

        # Identify missing keywords
        missing_keywords = self.identify_missing_keywords(cv_keywords, job_keywords)

        # Find matching keywords
        matching_keywords = [kw for kw in job_keywords if kw in cv_keywords]

        # Prepare report
        report = {
            "match_percentage": match_percentage,
            "cv_keywords": cv_keywords,
            "job_keywords": job_keywords,
            "matching_keywords": matching_keywords,
            "missing_keywords": missing_keywords
        }

        return report

    def print_report(self, report):
        """Print human-readable report"""
        print("\n" + "="*50)
        print(f"📊 CV ANALYSIS REPORT")
        print("="*50)

        print(f"\n🎯 MATCH PERCENTAGE: {report['match_percentage']:.2f}%")

        print("\n✅ MATCHING KEYWORDS:")
        for i, kw in enumerate(sorted(report['matching_keywords']), 1):
            print(f"  {i}. {kw}")

        print("\n❌ MISSING KEYWORDS:")
        for i, kw in enumerate(sorted(report['missing_keywords']), 1):
            print(f"  {i}. {kw}")

        # Calculate keyword distributions
        total_job_keywords = len(report['job_keywords'])
        matched = len(report['matching_keywords'])
        missing = len(report['missing_keywords'])

        if total_job_keywords > 0:  # Avoid division by zero
            print("\n📈 KEYWORD ANALYSIS:")
            print(f"  • Job description contains {total_job_keywords} important keywords")
            print(f"  • Your CV matches {matched} keywords ({(matched/total_job_keywords*100):.2f}%)")
            print(f"  • Your CV is missing {missing} keywords ({(missing/total_job_keywords*100):.2f}%)")
        else:
            print("\n📈 KEYWORD ANALYSIS:")
            print("  • No relevant keywords found in job description")

        print("\n💡 RECOMMENDATION:")
        if report['match_percentage'] >= 75:
            print("  Excellent match! Your CV is well-aligned with this job description.")
        elif report['match_percentage'] >= 50:
            print("  Good match! Consider adding some of the missing keywords to improve further.")
        else:
            print("  Your CV could be better aligned with this job. Try to incorporate more of the missing keywords.")

        print("\n🔍 SUGGESTED IMPROVEMENTS:")
        if missing:
            print("  Consider adding these top missing keywords to your CV:")
            for kw in sorted(report['missing_keywords'])[:5]:
                print(f"  • {kw}")
        else:
            print("  Your CV covers all important keywords from the job description!")

        print("="*50 + "\n")

    def save_report_to_file(self, report, output_file="cv_analysis_report.txt"):
        """Save report to a text file"""
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write("CV ANALYSIS REPORT\n")
            f.write("="*50 + "\n\n")

            f.write(f"MATCH PERCENTAGE: {report['match_percentage']:.2f}%\n\n")

            f.write("MATCHING KEYWORDS:\n")
            for i, kw in enumerate(sorted(report['matching_keywords']), 1):
                f.write(f"{i}. {kw}\n")

            f.write("\nMISSING KEYWORDS:\n")
            for i, kw in enumerate(sorted(report['missing_keywords']), 1):
                f.write(f"{i}. {kw}\n")

            # Calculate keyword distributions
            total_job_keywords = len(report['job_keywords'])
            matched = len(report['matching_keywords'])
            missing = len(report['missing_keywords'])

            f.write("\nKEYWORD ANALYSIS:\n")
            if total_job_keywords > 0:
                f.write(f"• Job description contains {total_job_keywords} important keywords\n")
                f.write(f"• Your CV matches {matched} keywords ({(matched/total_job_keywords*100):.2f}%)\n")
                f.write(f"• Your CV is missing {missing} keywords ({(missing/total_job_keywords*100):.2f}%)\n")
            else:
                f.write("• No relevant keywords found in job description\n")

            f.write("\nRECOMMENDATION:\n")
            if report['match_percentage'] >= 75:
                f.write("Excellent match! Your CV is well-aligned with this job description.\n")
            elif report['match_percentage'] >= 50:
                f.write("Good match! Consider adding some of the missing keywords to improve further.\n")
            else:
                f.write("Your CV could be better aligned with this job. Try to incorporate more of the missing keywords.\n")

            f.write("\nSUGGESTED IMPROVEMENTS:\n")
            if missing:
                f.write("Consider adding these top missing keywords to your CV:\n")
                for kw in sorted(report['missing_keywords'])[:5]:
                    f.write(f"• {kw}\n")
            else:
                f.write("Your CV covers all important keywords from the job description!\n")

        print(f"Report saved to {output_file}")

def main():
    analyzer = CVAnalyzer()

    print("CV to Job Description Analyzer")
    print("==============================")

    # Get CV file path
    cv_path = input("Enter path to your CV (PDF or TXT): ")

    # Get job description
    print("\nPaste job description (press Enter twice when done):")
    lines = []
    while True:
        line = input()
        if line.strip() == "":
            break
        lines.append(line)
    job_description = "\n".join(lines)

    # Generate and print report
    try:
        report = analyzer.generate_report(cv_path, job_description)
        analyzer.print_report(report)

        # Ask if user wants to save report
        save_report = input("Do you want to save this report to a file? (y/n): ")
        if save_report.lower() == 'y':
            output_file = input("Enter output filename (default: cv_analysis_report.txt): ")
            if not output_file:
                output_file = "cv_analysis_report.txt"
            analyzer.save_report_to_file(report, output_file)

    except Exception as e:
        print(f"Error: {e}")

if __name__ == "__main__":
    main()